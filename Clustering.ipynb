{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-458d6d962b540ab6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6: Unsupervised Learning: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are going to investigate some of the functionality provided by Scikit-learn for performing unsupervised learning, specifically, clustering. We will use the World Happiness Report (WHR) data set and see to what extent we can group different countries together (into clusters) based solely on the features contained in the  data set.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build your DataFrame and define your ML problem\n",
    "2. Prepare your data by cleaning the data and performing feature engineering\n",
    "3. Perform KMeans Clustering and analyze the results\n",
    "4. Perform Hierarchical Clustering and analyze the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Build Your DataFrame and Define Your ML Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "Rather than working with all of the data, we will just examine the data from 2015-2017, which we will store in a DataFrame named ```df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data_clustering\", \"WHR2018Chapter2OnlineData.xls\")\n",
    "\n",
    "df = pd.read_excel(filename, sheet_name='Table2.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>:  Inspect the data in DataFrame `df` by printing the number of rows and columns, the column names, and the first ten rows. You may perform any other techniques you'd like to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the ML Problem \n",
    "\n",
    "Recall that Unsupervised Learning works with unlabeled data. Therefore, we do not have to select a label for our machine learning problem. \n",
    "\n",
    "Let's define our problem. We are interested in how similar different countries are to each other, based on their features.  Are there natural groupings or clusters of countries based upon similarity of their feature values?  This is the kind of question that the machine learning technique of clustering addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Features\n",
    "\n",
    "We will use start by using the following columns as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['country', 'year', 'Life Ladder', \n",
    "                   'Positive affect','Negative affect',\n",
    "                   'Log GDP per capita', 'Social support',\n",
    "                   'Healthy life expectancy at birth', \n",
    "                   'Freedom to make life choices', \n",
    "                   'Generosity', 'Perceptions of corruption']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Modify DataFrame `df` to only contain the feature columns listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>:  Let's change the more complex feature names to ones that are simpler.  Use the dictionary containing old names and new names below, along with the Pandas `rename()` method, to change the names of the columns. For more information on the `rename()` method, consult the online [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names_dict = {'Life Ladder': 'Happiness', \n",
    "            'Log GDP per capita': 'LogGDP', \n",
    "            'Social support': 'Support', \n",
    "            'Healthy life expectancy at birth': 'Life', \n",
    "            'Freedom to make life choices': 'Freedom', \n",
    "            'Perceptions of corruption': 'Corruption', \n",
    "            'Positive affect': 'Positive', \n",
    "            'Negative affect': 'Negative'}\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one more piece of information that we'd like to include in our feature list. This is the region to which each country belongs, such as \"Central and Eastern Europe,\" and \"Latin America and Carribean.\" The WHR contains this information. Let's extract this info from the `SupportingFactors` worksheet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.read_excel(filename, sheet_name='SupportingFactors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Modify DataFrame `df_regions` to only contain the feature columns `country` and `Region indicator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect DataFrame `df_regions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>:  Change the name of the `Region indicator` column to  `Region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Merge DataFrame `df_regions` with DataFrame `df` based on the common column `country`. Use the Pandas `merge()` method. For more information on `merge()`, consult the online [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Print a list of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set the index of DataFrame `df` to be the country name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('country', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Prepare Your Data\n",
    "\n",
    "Now that we have identified our features, let's perform data preparation techniques to prepare our data for modeling.\n",
    "\n",
    "You will first clean your data by handling missing values and will then perform feature engineering by performing data aggregation and scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Check if DataFrame `df` contains missing values, and sum up the resulting values by columns. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Remove all examples (rows) that contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Inspect DataFrame `df` to see the if it still has missing values by once again summing up the missing values by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Data Aggregation\n",
    "\n",
    "Data aggregation is also called the summarization of data. We can perform data aggregation by performing aggregate functions on a group of data to create new data. For example, we can merge examples (rows) in a DataFrame by replacing these examples with one example that contains the mean or sum of the different feature values in the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have multiple examples (rows) per country, each containing information for different years. We can group examples together by country and merge them so that we can have one example per country. Let's do so by computing the mean values of all the features for each country, averaging over all years in the data set. Our resulting DataFrame `df` will then contain one example per country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['country', 'Region']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> We no longer need the `year` column. Remove the `year` column from DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the Data\n",
    "\n",
    "Let's scale the numerical data to normalize each column to have zero mean and unit standard deviation.\n",
    "\n",
    "We will use Scikit-learn's `StandardScaler` to accomplish this. Use the online [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) as a reference for how to use `StandardScaler`.\n",
    "\n",
    "First, let's import `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, do the following:\n",
    "\n",
    "1. Create a `StandardScaler` object by calling `StandardScaler()`. Save the result to variable `scaler`.\n",
    "2. Extract numerical features from DataFrame `df` and save the feature columns to DataFrame `df_to_scale`.\n",
    "3. Call the `scaler.fit_transform()` method to fit the scaler to `data_to_scale` and transform the data. Save the result to `transformed_data`.\n",
    "4. Call `pd.DataFrame()` to create a new DataFrame. Name the DataFrame `df_scaled`. Pass `transformed_data` as an argument. Specify the following parameters:\n",
    "    * `columns`: the column parameter should be given the column names from `df_to_scale` \n",
    "    * `index`: the index parameter should be given the value of `df_to_scale.index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a StandardScaler object and save the result to variable scaler.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# 2. Extract numerical features from DataFrame df and save it to DataFrame df_to_scale.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# 3. Call the scaler.fit_transform() method to fit the scaler to data_to_scale\n",
    "# and tranform the data. Save the result to transformed_data.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "#4. Create new DataFrame df_scaled\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Inspect df_scaled\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70aa3d38bc25a2ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3. Visualize the Data\n",
    "\n",
    "The `df_scaled` DataFrame contains, for each country listed, a group of nine numerical features.  The `Region` feature column from ```df``` has been removed because that is categorical rather than numerical.  Since we have scaled the data to have zero mean, any numerical entry that is greater than zero indicates a value above average, and any entry that is less than zero indicates a value below average.  Therefore, we can think of each country as being \"defined\" by this group of nine features.\n",
    "\n",
    "We are interested in how similar different countries are to each other, based on their feature values.  \n",
    "\n",
    "To begin, let's have a visual look at the data, which consists of 152 rows x 9 columns.  The code cell below uses the seaborn `heatmap()` function to make a heatmap of `df_scaled`.\n",
    "\n",
    "The default rendering of the heatmap is not ideal, being somewhat short and squat when the dataset is much longer.  To get around this, we first create a figure with a size that is better suited to this plot, and then make the call to `heatmap()`.  \n",
    "\n",
    "Since we have requested a large figure size, it is possible that the figure might be embedded in a scrollable sub-window rather than rendered in its entirety in the notebook as is usually the case. If you'd like to be able to expand the sub-window and get the figure embedded fully within the notebook, you can click on the panel to the left of the figure (under the Out[] indicator, i.e., to the left but still within the notebook). If you decide you want to convert it back again to a scrollable sub-window, you can click in that left panel again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14da59a809f6e48c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,30))\n",
    "sns.heatmap(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc555da20146977b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 4. Perform KMeans Clustering and Analyze the Results\n",
    "\n",
    "The countries in the heatmap you have just created are ordered alphabetically, leading to what looks like a random pattern of feature values as you scan down the heatmap.  The point of clustering is to pull out subgroups (clusters) that share similar feature profiles.\n",
    "\n",
    "The first clustering algorithm we will examine is KMeans (sometimes written as k-means or K-means), which is described in more detail in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html#k-means). \n",
    "\n",
    "With KMeans, one specifies in advance how many clusters one would like to group the data into, and this value is the number $K$, hence the name of the method.  Sometimes one needs to experiment with different values of $K$ to see what best reflects the nature of the data. KMeans works by finding a set of \"centroid\" points at the center of each cluster, so that every data point (example) within a cluster is closer to the center of its own cluster than to the centroid of a different cluster.\n",
    "\n",
    "In a hypothetical dataset with $N$ examples, a \"perfect but meaningless\" clustering of the data would involve making as many clusters as there are examples, that is, setting $K = N$.  In such a scenario, every example (e.g., every country in our dataset) would be in its own cluster, perfectly identical to itself, but meaningless in terms of revealing structure and patterns in the data.  In the opposite limit, $K = 1$, all the examples would be in the same cluster, which leaves us no better than where we started.  The point of clustering is to find a number of clusters $K$ (somewhere between $1$ and $N$) that captures substructure within the data.\n",
    "\n",
    "In the case of our WHR data, a useful guess for $K$ might reflect the number of different regions that are contained in  DataFrame `df`. In this exercise, we want to cluster countries based on the WHR features, and might be interested in how well the clustering reflects the region data.  Thus, a useful place to start here is to set $K$ to be the number of unique regions contained in DataFrame `df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Determine the Number of Clusters $K$\n",
    "Let's first figure out how many clusters we want to find.\n",
    "\n",
    "<b>Task:</b>\n",
    "\n",
    "* Find the number of unique values contained in the `Region` feature in DataFrame `df`. Save the results to  variable`K`.\n",
    "* Print the value of `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ac4b26dee13c65e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b3e043c4e452a4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 2: Apply KMeans Clustering to the Data\n",
    "\n",
    "Having identified the number of clusters we want to look for, let's continue with Kmeans. \n",
    "\n",
    "First let's import Scikit-learn `KMeans` class. For more information, consult ```KMeans``` Scikit-learn online [dcocumentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below:\n",
    "\n",
    "1. Create a ```KMeans```model object, specifying the parameter `n_clusters` to contain the number of clusters $K$, and assign the results to the variable `kcluster_model`. \n",
    "\n",
    "2. Fit the model to `df_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59cf76dd60bd7f16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e6bea92d09ee49cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The output from the code cell above, if entered and run correctly, should note some further information about the `KMeans` clusterer that you have created and fit to the data.  There are various other options that can in principle be specified in constructing the object; the only option we specified above was ```n_clusters```, and used the default values of everything else.  \n",
    "\n",
    "You will notice an entry for ```n_init=10```.  This refers to the fact that the Kmeans algorithm starts from a initial guess for the centroid locations, and runs the algorithm repeatedly with different starting guesses (`n_init` times), choosing the best clustering from the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Label Each Example with the Cluster It Belongs To\n",
    "\n",
    "Once the `KMeans` clusterer has fit the data, it contains various data attributes that we can query. One of these attributes is the ```labels_``` object, which is an array of integers that assigns a cluster label to each example in the dataset.  For our WHR data, therefore, the ```labels_``` object describes, in the same order in which the data appears in our ```df_scaled``` DataFrame, which cluster each country belongs to.\n",
    "\n",
    "The code cell below prints the value of ```kcluster.labels_```. Run the cell and examine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b75cacf7418e7e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(kcluster.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-50123c25d42eca79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The actual numbers associated with the cluster labels are meaningless, and will change each time you run the clustering algorithm.  (By this we mean that we could permute the labels of all the clusters, and the underlying clustering of the data would remain unchanged.)  What is meaningful, however, is which countries *share* the same cluster label, since that means they are assigned to the same cluster (regardless of what cluster label number that is).  Since the cluster labels printed above contain no information about the countries they correspond to, it is useful to combine the cluster labels with the DataFrames that they originated from.  We'll do that by creating a copy of both the ```df``` and ```df_scaled``` DataFrames, and adding information about the cluster labels.\n",
    "\n",
    "<b>Task:</b> In the code cell below:\n",
    "\n",
    "1. Make a copy of DataFrame `df` using its `copy()` method. Assign the result to a new DataFrame named `df_clustered`.\n",
    "2.  Add a new column named `klabel` to `df_clustered`. This column should contain the values in `kcluster.labels_` \n",
    "3. Make a copy of DataFrame `df_scaled` and assign the result to a new DataFrame named `df_scaled_clustered`.\n",
    "4. Add a new column named `klabel` to `df_scaled_clustered`. This column should contain the values in `kcluster.labels_` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a6c2c6e90202ea2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below and examine the new ```df_clustered``` DataFrame.  You should see the `klabel` column now &mdash; any two countries with the same label have been assigned to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-54a456427b830d5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Step 4. Analyze the Results\n",
    "\n",
    "Recall that each cluster is approximately defined by its centroid, which we can think of as a typical or average feature profile for that cluster.  Our examples actually live in a 9-dimensional space (9 numerical features), so it's a little hard to visualize the data.  But if we want to see what each of the clusters approximately \"looks like,\" we can just plot the coordinates of the cluster centroids, which are stored in the attribute ```kcluster.cluster_centers_```.\n",
    "\n",
    "Execute the code cell below to make a plot of the cluster centers and their associated label numbers.  Note that the cluster centers are all distinct from one another, although clusters are somewhat closer to each other.  This might be indicative of the fact that $K=10$ might be too many clusters to properly represent this dataset; if $K$ were reduced, some of the nearby clusters would probably collapse into a larger metacluster reflecting their approximately common profile.  We'll just examine the $K=10$ case in this exercise, but if you're interested in investigating further on your own, you could try some other values of $K$ to see how the results change.\n",
    "\n",
    "Feel free to modify the code if you want to tweak the plot, or to understand what each of the commands there is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Happiness', 'LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption', 'Positive', 'Negative']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e7d3cd31827e0ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.plot(kcluster.cluster_centers_[i], label=i)\n",
    "plt.xticks(range(9), numerical_features, rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-700d62498ad400c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the plot above, each line represents the centroid of one cluster (for clusters labeled 0 through 9).  For each cluster, there is a group of countries whose features lie near to these centroids.\n",
    "\n",
    "First examine, and then execute, the code cell below.  \n",
    "\n",
    "The code cell below defines a function called ```plot_cluster_and_centroid```, which takes as input an integer cluster label, and plots all country profiles within the cluster (in different colors) along with the centroid profile (plotted with a black dashed line and black point markers).\n",
    "\n",
    "After the function definition, the function is called to produce a plot for cluster label 0.\n",
    "\n",
    "<b>Task:</b> Modify the code cell below to plot all Clusters. Once you've examined the plot for cluster 0, modify the cluster label being passed to the function in order to view each of the other 9 clusters that have been produced (10 clusters in all, since we set $K = 10$).  In other words, modify the input to the function one at a time, and re-execute the code cell, or &mdash; if you prefer &mdash; open up additional code cells below to plot each of the clusters in turn. At present, the plot turns off the legend indicating the names of the countries in each cluster (```legend=False```), since the legend clutters the figure for large clusters and is difficult to reposition.  But if you're curious, try turning the legend back on to see the contents of each cluster.\n",
    "\n",
    "Take some time to examine these plots to understand what they are conveying.  Some clusters represents countries that might have greater `Happiness` and lower `Corruption`, for example, whereas others might represent different combinations of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-512f71b717565b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_cluster_and_centroid(label):\n",
    "    df_scaled_clustered[df_scaled_clustered.klabel==label][numerical_features].T.plot(legend=False)\n",
    "    plt.plot(kcluster.cluster_centers_[label], 'ko--')\n",
    "    plt.xticks(range(9), numerical_features, rotation=45)\n",
    "\n",
    "# Call plot_cluster_and_centroid() multiple times with each cluster label (0-9) as an argument \n",
    "# and analyze the resulting plots\n",
    "plot_cluster_and_centroid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da2b3f00173c1001",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We might be interested in how the results produced by KMeans clustering relate to the region information that is stored in DataFrame ```df_clustered```.  As we discussed above, the cluster labels themselves are meaningless, but we can easily examine how each cluster label (`klabel`) aligns with different world regions.  For example, we might want to know how many countries from each region are associated with cluster number 0, cluster number 1, etc.\n",
    "\n",
    "The code cell below contains an expression to group the data in ```df_clustered``` by both `klabel` and `Region`, and compute the size of each (`klabel`, `Region`) pair. It is done in one line using the ```groupby()``` method, along with the ```size()``` method that is applied to each group produced by the `groupby` operation.  An operation of this sort will produce a new DataFrame with a two-level MultiIndex (i.e., each row described by its (`klabel`, `Region`) pair), with the corresponding column indicating how many countries are associated with that pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed8e8354f186aa5c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_clustered.groupby(['klabel', 'Region']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7d59470a1332940",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 4. Perform Hierarchical Clustering and Analyze the Results\n",
    "\n",
    "\n",
    "The Kmeans algorithm is just one of many clustering algorithms supported by Scikit-learn.  A colorful overview of different clustering methods is provided in the Scikit-learn online [documentation](https://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "Another widely used clustering method is known as [Hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), some variants of which are known as agglomerative clustering.  This method takes a slightly more nuanced approach to the process of clustering.  Whereas KMeans clustering specifies a fixed number of clusters to group data into, hierarchical clustering develops a \"hierarchy\" of clustering relationships where data points are grouped more closely together if they are more similar to each other.  It's somewhat like the way that ancestry works.  If someone were to ask you how many people you are related to, you might reply that it depends on how far back they want to go in time to define \"relatedness.\" Presumably we are all related to each other (however remotely) if we go back far enough in time to a common evolutionary ancestor, but that fact might not be so useful in defining \"relatedness\" or \"clusters\" of people.  Nonetheless, hierarchical clustering does provide insight into these hierarchical relationships among clusters, and lends itself to useful visualization techniques that reveal those relationships.\n",
    "\n",
    "Scikit-learn provides objects and methods for `AgglomerativeClustering` that operate similarly to the way that the KMeans object worked above, by creating a clustering object and then fitting it to the dataset of interest.  If you're interested, you can investigate this further by following along with the Scikit-learn documentation.  Unfortunately, Scikit-learn does not provide great support for plotting the results of hierarchical clustering, generally referred to as dendograms.  There is some code showing a [simple example](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html).\n",
    "\n",
    "Instead, we can work with `seaborn`, which provides a useful function called `clustermap()` that does two things: it performs hierarchical clustering and then displays the results.  The `clustermap()` function is described in the seaborn online [documentation](https://seaborn.pydata.org/generated/seaborn.clustermap.html#seaborn.clustermap).  The function  uses a version of the clustering algorithm that is included as part of [scipy](http://scipy.org), a Python package that provides a wealth of useful tools for scientific computing.\n",
    "\n",
    "The code cell below contains a call to `sns.clustermap()` to operate on DataFrame `df_scaled`, providing the following additional options:\n",
    "\n",
    "* ```method = 'average'```\n",
    "* ```metric = 'euclidean'```\n",
    "* ```figsize = (15,40)```\n",
    "\n",
    "Note that the data is clustered inc`df_scaled`.  You wouldn't want to cluster the data in the augmented DataFrame that you created above (```df_scaled_clustered```), because that contains additional information about the KMeans cluster labels that you don't want to include here.\n",
    "\n",
    "The code constructs and displays a figure that looks a bit like the heatmap that we created earlier in this exercise, but with some differences.  As was discussed above, if the figure is in a sub-window that you'd like to expand, you can click on the panel to the left of the figure to do so.\n",
    "\n",
    "After you have examined the figure, proceed to the material below the figure to read further about what is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b37e15e123f3b721",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['Happiness', 'LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption', 'Positive', 'Negative']\n",
    "\n",
    "sns.clustermap(df_scaled[numerical_features], method='average', metric='euclidean', figsize=(15,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc7c1efcb9f47f72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are several things you should notice about the plot produced by the clustermap.  First, whereas our original heatmap ordered the countries alphabetically, with a more or less random pattern of indicator values as a result, this clustermap has reordered the rows to reflect the clustering.\n",
    "\n",
    "You should also notice a tree-like structure running along the left-hand side of the heatmap.  This tree, also known as a dendogram, is what is providing information about the clustering.  It is a tree, because it starts from a broad trunk at the far left of the dendogram down to finer and finer sub-branches and finally down to individual \"leaves\" that represent individual countries.  The clustering algorithm works from the bottom up: It finds several pairs of countries that are highly similar within each pair (smallest euclidean distance between them, in this version of the analysis), and then finds other countries that are similar to each pair.  Thus the algorithm grows clusters by agglomerating onto groupings that have already been identified.  The similarity between two countries in the tree is reflected by how far you need to go \"up\" the tree from one country and then back \"down\" to the other one.  If you go far enough up the tree, all countries are similar enough to each other to be grouped in one big cluster, similar to how all people are related to one another if we go back far enough in time.  Two countries that are near each other vertically in the reordered heatmap tend to be more similar to each other, but that is not strictly the case.  You should notice that at particular parts of the dendogram different branches end up getting placed nearby each other in this 2D representation, but their distance from each other up and back down the tree could be very far.\n",
    "\n",
    "There are some other things to notice in this plot.  \n",
    "\n",
    "First, despite the fact that nearby ordering in the heatmap does not always reflect close similarity in the tree, you should be able to discern some clustering visually in the reordered heatmap, that is, groups of countries with similar WHR feature values.  If you see a group of countries with similar feature values (similar patterns of colors in the heatmap), you should be able to identify the sub-tree on the left that group is associated with.\n",
    "\n",
    "Second, you should notice that the columns of the dataset have also been clustered, with their own dendogram running along the top.  This is indicative of the fact that some groups of features are more closely associated among themselves, such as `LogGDP` and `Life` (life expectancy) which are clustered more closely together.  Clustering along both axes of a dataset is known as \"biclustering\", and is turned on by default in ```clustermap()```.  If you want to cluster only along one axis, you could modify either the `row_cluster` or `col_cluster` options to the function.\n",
    "\n",
    "Finally, we have chosen just one set of criteria to carry out this clustering (```method='average'``` and ```metric='euclidean'```).  In order to identify groups of similar items in a dataset, we need to specify mathematically what we mean by \"similar.\"  In this case, we have defined the similarity of two examples based on their euclidean distance from each other (such that two identical items would be separated by zero distance).  But we could have chosen some other criterion instead, such as the correlation between two examples.  In addition, in hierarchical clustering, one needs to specify not only how similar two *data points* are, but also to specify how similar two *clusters* are (since it is building up clusters of clusters).  This is what the `method` parameter is about (or more specifically, what is called the \"linkage method\").  The similarity between two clusters might be based on how similar their two closest items are, or their two most distant items, or the average distance between all pairs of points in each cluster.\n",
    "\n",
    "If you're interested, you can consult the documentation for `clustermap()` and experiment with some different options to explore the effect of different clustering metrics and methods.  There is no one right answer, but generally you'd like to find clustering results that exhibit some degree of robustness to variations in these sorts of options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
